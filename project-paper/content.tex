% status: 0
% chapter: TBD

\title{Analyzing Twitter Activity to Identify Spam Accounts}


\author{Stephen Giuliani}
\affiliation{%
  \institution{Indiana University}
  \state{Virginia}
  \country{USA}
}
\email{sgiulian@umail.iu.edu}

\renewcommand{\shortauthors}{S. Giuliani}


\begin{abstract}
This paper provides an analysis of Twitter account activity to identify potential spamming bots as opposed to organic user activity. The approach uses a combination of web scraping in python, data storage within the MongoDB Atlas Cloud, and implementing a REST service to pull data from the cloud for analysis.
\end{abstract}

\keywords{hid-sp18-507, Twitter, MongoDB, REST, TweePy}


\maketitle

\section{DRAFT}

DRAFT as of: 03Apr2018

This copy is a draft of the final paper. This is incomplete as it does not have its full content, links, citations, graphics, or other elements. This version may not compile into a clean PDF. This draft version will be updated periodically.

\section{Introduction}

The 2016 US Presidential election is a prime example of how social media can be used to push an agenda or advertise specific points of view in such a volume that descrediting the masses of posts, shares, and re-tweets is a near impossible feat. Many individuals, organizations, as well as government entities, have utilized social media to flood various social mediums with repetative posts and comments at the hands of automated bots, rather than real-user activity. Since the unethical use of social media in this manor has come to light, post election, the US Department of State (DoS) and the Federal Bureau of INvestigation (FBI) has been tasked at identifying and addressing the use of fake accounts to push political agendas.

Luckily for investigators, account activity for automated or fake users is typically characteristically different from the activity of organic or real users. This paper will provide an analysis on Twitter users' accounts and user-habits to identify potential bots. This paper will also identify certain topics that are more likely to fall victim to the spaming tactics than less polar subject matter.

\section{Bots, Spam, and the Impact}

Automation is a norm in technology, especially social media. The purpose of the automated account, or bot, is what determines whether the account is considered spam versus a service. Examples of more favorable automated accounts include the `Earthquake Bot' (`@earthquakebot'), which automatically tweets warning information to areas where a 5 or greater magnitude earthquake is detected, the `Netflix Bot' (`@netflix_bot'), which will notify you each morning of the latest Netflix releases, and the `Big Ben Clock' (`@big_ben_clock'), which just tweets `BONG' everytime London's Big Ben chimes. The fun and useful bots are just a few examples of how Twitter accounts can be used for a collectively positive purpose. In contrast, there are accounts that serve only to like, re-tweet, or even produce original content, at a higher daily rate and often focus on subject that are hot-topics in the news or polarized views in politics. Within this analysis, we have concluded that certain topics are more likely to have a spam bot presence than others. For instance, `#LockHerUp', a notorious hashtag used in favor of putting Hillary Clinton in jail, is used by bots significantly more than `#SundayBrunch'.

The accounts that can be considered spam-based bots have similar account behavior to other bots and can be classified collectively via just a few data points. A spam bot is more likely to post at a much higher volume than a real user. Some bots rack up high counts of re-tweets with very few original posts, have a very unballanced ratio of followers to people who follow back, and may achieve a high volume of activity within a few days of creation.

The use of these spaming accounts cretes artificial interest, or hype in specific subjects. This tactic can keep topics in the limelight or falsly overpower reputable news sources ultimately controlling what we see online or in mainstream media. Instances where the masses latched onto the volume of information rather without investigating source or information independantly can lead to dangerous conclusions and sometimes unjustified courses of action. In 2012, the moments immediately following the unfortunate incident at Sandy Hook lead to an online hunt and prosecution of Ryan Lanza. Varieties of social mediums were quick to post and share than Ryan was the perpretrator os the terrorist attack, including death threats and pledges of revenge. However, as a short time passed and sources were verified and facts pushed through the social hype, Adam Lanza, Ryan's brother, was identified as the shooter--well after news outlets followed and falsly reported on socially-based conclusions.

\section{Analytical Approach}

\subsection{Technologies Used}

 * Twitter API
Twitter uses a publically available API for all of its users' Twitter activity and profile information. Free developer accounts are available and come with the ability to pull data from the past 7 days. Increased scope is available through paid-tiers of developper accounts.

 * Python: tweepy
 Tweepy is a package developped for the purpose of interacting with Twitter's online API. Queries are returned in JSON format for analysis or direct storage.

 * MongoDB Atlas
 MongoDB is a popular NoSQL document database program that is open source and can be deployed almost anywhere. MongoDB Atlas is a cloud-based MongoDB service that can host documents through the services of Amazon Web Services (AWS), Microsoft's Azure, or Google's Cloud Platform (GCP). For purposes of this paper and analysis, we have used MongoDB Atlas to host the scraped Twitter data, using AWS, and expose the data through a RESTful service.

 * Python: Flask (REST)
 Flask is a micro framework for Python that can be used to design and make RESTful queries. We use flask to make GET requests to the MongoDB Atlas Cloud data. The data requested is used for the analysis itself.

 \subsection{Part 1: Web Scraping, Twitter Data}

 In order to pull data from Twitter's API, you must first register for a devloper account (free). Once registered, and after creating a defaul application, the account keys and credentials are necessary for authentication when usiny tweepy in python. A free developer's account has the ability to pull twitter data from the past 7 days only. For purposes of this analysis, these seven days are sufficient. Documentaion on the data Twitter makes available to the public is available here (https://developer.twitter.com/en/docs/api-reference-index). Using tweepy, we are able to query Twitter's data via a number of characteristics: hashtags, words present in tweets, usernames, dates, and more. The JSON object that is returned (dependant on the structure of the query) includes information on the user's account since creation as well as meta-data on the tweet itself (geotags, platform used to post, links used, and more). The entirety of the returned JSON object is what is stored on the MongoDB Atlas cloud.

 A typical Twitter query JSON object is less than 8 kB at a length of about 212 lines of code, based on a JSON dump of 100 tweet-data returns. Therefore, a high volume of twitter data can be pulled in stored within the cloud for analysis.

 \subsection{Part 2: MongoDB Atlas, Storing Twitter Data}

 MongoDB's online web-hosting cloud service is available for free for testing purposes. The limitations of this service is a 3-cluster database with a storage limit of 512 Mb. Any requirements to use more than 3 clusters or storage needs greater than 512 Mb requires paid subscriptions, which are hourly-usage based. The data scraped from Twitter will be stored on this cloud and will be available to readers/reviewers after the analysis is concluded.

 The ability to store data on a MongoDB database comes from the use of pymongo, a Python package developped for the administration of Mongo databases. However, for testing purposes, we also used the mongo-shell capabilities to clear, start, stop, and monitor the cloud-server status. Documentation on pymongo and mongo-shell are available here (https://api.mongodb.com/python/current/)

 Connection to the cloud database requires ssl authentication, which will be published within this paper for testing purposes. Administration or viewing permissions will also be created. The JSON objects produced within Python using tweepy are stored in a creaded database collection on the Atlas cloud. This cloude database is also use as the connection for the RESTful service to pull specific data-points.

 \subsection{Part 3: Flask, RESTful Service in Pulling from the Cloud}

 The data or JSON-format documents stored on the Atlas cloud can be cennected to and authenticated via pymongo in python. Flask is used to bridge the connection and expose an available address for data to be displayed/used for analysis. The RESTful api and paths created were created from scratch, grouping specific JSON attributes of the stored data. For instance, in order to determine the daily rate of posts per user, the GET request will pull the data for the user's total posts and the data including the creation date of the account. The JSON object produced via the GET request can be parsed and regrouped prior to exposing the data on flask's default local endpoint. See the code-documentation for the REST API used to query the Atlas cloud.

 \subsection{Part 4: Data Analysis and Results}

 As mentioned above, the data on the cloud can be pulled via GET requests. The data required is pulled via specific GET requests so that analysis can be done. MongoDB's Atlas cloud service has the ability to perform analysis on the cloud; however a free testing account is not able to perform this. Therefore, the analysis is conducted locally, via python objects. For purposes of data familiarization, the data is pulled and stored in a comma-separated values (CSV) format. This enables seemles interpretation by python's many analytical and visual packages. Metrics such as daily rates for friending other users and posting, friends-to-followers ratios, and account age are calculated. Medium, a popular online publication and editorial provides ``Twelve Ways to Spot a Bot'' (https://medium.com/dfrlab/botspot-twelve-ways-to-spot-a-bot-aedc7d9c110c) on Twitter; many of these points can be verified via Twitter's public API data.

\section{Analysis: challenges}

This paper demonstrates the ability to scrape...

\section{##############################}

\section{Quotes}

Do not use double quotes \verb|"| but use \LaTeX\ ``quotes''. Quotes
{\bf MUST} not be used to highlight works. Quotes are {\bf STRICTLY}
used for quoting text from sources with citation following. If we find
a quote that is not followed by a citation we will return the paper
without review.

\section{Labels}

Do not use actual numbers in the taxt after you write for example
Figure 1 use the ref for the figure while using its label. In our
example it is Figure~\ref{f:fly} and Table Figure~\ref{t:mytabble}.
See the source for the example.

\section{Footnotes}

Footnotes must be avoided in papers. All URLs must be included as full
references and citations and used with the \verb|\cite| command
\footnote{do not use footnotes}. You {\bf must} not use urls in the
text or paper.

\section{Plagiarism}

The class includes a section about plagiarism which you must adhere
to. Copying text without proper citation is considered cheating and we
will assign the grade ``F'' for the paper if we find you do it. It is
in your responsibility to make sure plagiarism does not occur. Please
be aware that our checks are better than the once provided by turnitin
or other online checkers. Excuses such as ``I did not have time'' or
``I forgot'' can not apply as you have enough time to prepare the
paper and must not forget. 

\section{Check}

make sure just as in previous assignments that you check your paper
with chktex and lacheck. Fix the errors that you see. Some of the
errors may be ok, but in general make sure you address all of them. If
in doubt work with the TA. Simply use

\begin{verbatim}
make check
\end{verbatim}

We include in the handbook a list with common issues that we see when
students submit papers. One particular important issue is not to use
the underscore in bibtex labels. It is your responsibility to check
the paper for the issues indicated.

To check bibliographies simply use

\begin{verbatim}
pdflatex report
bibtex report
\end{verbatim}

You will see the errors and warning son the screen address them

TA's will in addition use a special test checking for additional
format issues such as detecting if you used labels and refs for
floats. You are welcome to also try this test, but we provide it
without explanation as no explanation is needed since if you followed
the instructions on floats there should be no issues. If you like to
do that test, you can use  

\begin{verbatim}
make check-ta
\end{verbatim}

\section{Convenient Setup}

If you do not have already a paper dir in your repository, here is a
way to create one. Replace the hid-sp18-000 with your hid.

\begin{verbatim}
export HID=hid-sp18-000
mkdir -p ~/github/cloudmesh-community
cd ~/github/cloudmesh-community
git clone https://github.com/cloudmesh-community/hid-sample.git
git clone https://github.com/cloudmesh-community/$HID.git
\end{verbatim}

Next copy the paper example

\begin{verbatim}
cp -r hid-sample/paper $HID
cd $HID
git add paper
git commit -m "add the paper directory" paper
git push
\end{verbatim}

Make sure there is no \verb|/| behind the paper in the cp command or you mess up the
copy process.


\section{Creating the PDF}

The PDF\ can be created simply with 

\begin{verbatim}
make clean
\end{verbatim}



{\bf UNDER NO CIRCUMSTANCES ARE YOU ALLOWED TO CHECK IN YOUR PDF OR
  TEMPORARY LATEX FILES INTO GITHUB. GIT NEEDS TO STAY CLEAN AND ONLY
  CONTAIN THE SOURCES.}

We will deduct points if you do violate this.

\section{Conclusion}

Put here an conclusion. Conclusion and abstracts must not have any
citations in the section.


\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

